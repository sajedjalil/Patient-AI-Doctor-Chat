from typing import Literal, List, Tuple, Optional, Union

from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, RemoveMessage, ToolMessage
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import START, StateGraph, MessagesState, END
from langgraph.prebuilt import tools_condition, ToolNode

from home.constants import constants
from home.constants.chat_models import model_claude_3_haiku
from home.constants.constants import summary_prompt, summarize_trigger_count
from home.function_tools.tools import Tools
from home.models.patient import Patient

# list of function calling tools that are used in this graph
tool_list = [
    Tools.request_medication_change,
    Tools.request_appointment_change
]


# Use of Message state from standard library
class State(MessagesState):
    summary: str


class LLMGraph:
    """
    A class to for langchain graph that has AI agents with summarization techniques to handle long context
    conversions and various function calling capabilities.
    """
    def __init__(self):
        """
        Initialize the LLMGraph class.

        This method sets up the necessary components for the LLM-based chatbot, including loading the patient data,
        selecting the language model, binding function calling tools, and compiling the state graph.

        Attributes:
        - patient: The first patient object retrieved from the database.
        - model: The selected language model, bound with the list of tools.
        - graph: The compiled state graph representing the chatbot's behavior.

        Returns:
        - None
        """
        self.patient = Patient.objects.first()  # Get current patient using first entry from database
        self.model = model_claude_3_haiku
        self.model = self.model.bind_tools(tool_list)
        memory = MemorySaver()  # for langsmith tracing
        self.graph = self.__build_graph().compile(checkpointer=memory)

    def chat_inference(self, user_message: str, history: List[dict], thread_id: str) \
            -> Tuple[str, Optional[str], List[str]]:
        """
        Performs the chat inference process by processing the user's message, invoking the LLM-based chatbot,
        and returning the assistant's response, summary, and the list of tools called during the conversation.

        Parameters:
        - user_message (str): The user's input message to be processed.
        - history (List[dict]): The conversation history, represented as a list of dictionaries,
          where each dictionary contains 'role' and 'content' keys.
        - thread_id (str): The unique identifier for the conversation thread. This is unique over all users and all
          conversations.

        Returns:
        - Tuple[str, Optional[str], List[str]]: A tuple containing the assistant's response (str), the summary of the
          conversation (Optional[str]), and the list of tools called during the conversation (List[str]).
        """
        config = {"configurable": {"thread_id": thread_id}}
        messages = self.__convert_history_to_messages(history)
        messages.append(HumanMessage(content=user_message))

        result = self.graph.invoke({"messages": messages}, config)

        assistant_response = result['messages'][-1].content
        summary = self.graph.get_state(config).values.get("summary", None)
        tools_called = [msg.content for msg in result["messages"] if isinstance(msg, ToolMessage)]

        return assistant_response, summary, tools_called

    def __ai_agent(self, state: State) -> dict:
        """
        The AI agent function that interacts with the language model to generate responses.

        This function takes a state object as input, which contains the current conversation history and summary.
        It prepares a system message with current patient information, appends it to the conversation history,
        and invokes the language model to generate a response.

        Parameters:
        - state (State): The current state of the conversation, containing the conversation history.

        Returns:
        - dict: A dictionary containing the generated messages. The dictionary has a single key-value pair,
          where the key is "messages" and the value is a list containing the AIMessage generated by the model.
        """
        sys_msg = (
            SystemMessage(content=constants.llm_prompt_text + "Currently, you are chatting with a patient with "
                                                              "the following information: " + self.patient.__str__()))
        return {"messages": [self.model.invoke([sys_msg] + state["messages"])]}

    def __build_summarize_subgraph(self) -> StateGraph:
        """
        Constructs a langchain subgraph for summarizing the conversation.

        This method initializes a StateGraph object with the State class, adds a node for the summarize_conversation 
        function, sets up conditional edges based on the if_need_summarization() function, and connects the nodes.

        Returns:
        StateGraph: A compiled StateGraph object representing the summarize subgraph.
        """
        builder = StateGraph(State)
        builder.add_node("summarize_conversation", self.__summarize_conversation)
        builder.add_conditional_edges(START, self.__if_need_summarization)
        builder.add_edge("summarize_conversation", END)
        return builder

    def __build_tool_call_subgraph(self) -> StateGraph:
        """
        Constructs a langchain subgraph for handling tool calls in the chatbot.

        This method initializes a StateGraph object with the State class, adds nodes for the AI agent and tools,
        sets up conditional edges based on the tools_condition function, and connects the nodes.

        Returns:
        StateGraph: A compiled StateGraph object representing the tool call subgraph.
        """
        builder = StateGraph(State)
        builder.add_node("ai_agent", self.__ai_agent)
        builder.add_node("tools", ToolNode(tool_list))

        builder.add_edge(START, "ai_agent")
        builder.add_conditional_edges("ai_agent", tools_condition)
        builder.add_edge("tools", "ai_agent")

        return builder

    def __build_graph(self) -> StateGraph:
        """
        Constructs the main langchain StateGraph for the chatbot.

        This method initializes a StateGraph object with the State class, adds nodes for the summarize and tool call
        sub-graphs, sets up conditional edges based on the defined sub-graphs, and connects the nodes.

        Returns:
        StateGraph: A compiled StateGraph object representing the chatbot's overall chain/structure.
        """
        builder = StateGraph(State)
        builder.add_node("summarization_subgraph", self.__build_summarize_subgraph().compile())
        builder.add_node("tool_call_subgraph", self.__build_tool_call_subgraph().compile())

        # Set up the flow of the chatbot: start with tool call subgraph, then summarize subgraph, and end
        builder.add_edge(START, "tool_call_subgraph")
        builder.add_edge("tool_call_subgraph", "summarization_subgraph")
        builder.add_edge("summarization_subgraph", END)

        return builder

    def __summarize_conversation(self, state: State) -> dict:
        """
        Summarizes the conversation by generating a summary based on the provided state.

        This function takes a state object as input, which contains the current conversation history.
        It appends a summary prompt to the conversation history, invokes the language model to generate a summary,
        and prepares a list of messages to be deleted from the conversation history.

        Parameters:
        - state (State): The current state of the conversation, containing the conversation history.

        Returns:
        - dict: A dictionary containing the summary and the list of messages to be deleted.
          The dictionary has two key-value pairs:
          - "summary" (str): The generated summary of the conversation.
          - "messages" (List[RemoveMessage]): A list of RemoveMessage objects representing the messages to be deleted.
        """
        messages = state["messages"] + [HumanMessage(content=summary_prompt)]
        response = self.model.invoke(messages)

        delete_messages = [RemoveMessage(id=m.id) for m in state["messages"][:-2]]
        return {"summary": response.content, "messages": delete_messages}

    @staticmethod
    def __if_need_summarization(state: State) -> Literal["summarize_conversation", "__end__"]:
        """
        Determines whether the conversation needs to be summarized based on the number of messages.

        This function checks the length of the conversation history stored in the state object.
        If the number of messages exceeds the predefined threshold (summarize_trigger_count), it returns
        "summarize_conversation". Otherwise, it returns "__end__" to indicate that no summarization is needed.

        Parameters:
        - state (State): The current state of the conversation, containing the conversation history.

        Returns:
        - Literal["summarize_conversation", "__end__"]: A string indicating the next action to be taken.
          If "summarize_conversation", the conversation should be summarized.
          If "__end__", no summarization is needed.
        """
        if len(state["messages"]) >= summarize_trigger_count:
            return "summarize_conversation"
        else:
            return "__end__"

    @staticmethod
    def __convert_history_to_messages(history: List[dict]) -> List[Union[HumanMessage, AIMessage]]:
        """
        Converts a list of dictionaries representing conversation history into a list of HumanMessage or AIMessage
        objects.

        This function takes history (a list of dictionaries) as input, where each dictionary represents a message in the
        conversation history. The dictionaries contain 'role' and 'content' keys. The function converts the 'user'
        role to a HumanMessage object and the 'assistant' role to an AIMessage object.

        Parameters: - history (List[dict]): A list of dictionaries representing conversation history. Each dictionary
          has 'role' and 'content' keys.

        Returns:
        - List[Union[HumanMessage, AIMessage]]: A list of HumanMessage or AIMessage objects representing the
          conversation history.
        """
        return [
            HumanMessage(content=msg['content']) if msg['role'] == 'user'
            else AIMessage(content=msg['content'])
            for msg in history
        ]
